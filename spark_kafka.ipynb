{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to run:   \n",
    "1. docker compose up  \n",
    "2. docker exec -it cpts415-kafka-1 /kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test\n",
    "3. Run code on this notebook: http://localhost:8888\n",
    "4. 'cd GUI' and run node app.js\n",
    "5. The app will send messages to Kafka. View them here (docker exec -it cpts415-kafka-1 /kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test)\n",
    "6. View Spark dataframe here: (docker logs (pid of notebook) -f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, lit, lead, concat, udf, from_json, expr\n",
    "from pyspark.sql import Window\n",
    "import random\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('spark://spark-master:7077').config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0').config(\"spark.executor.cores\", \"2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = \"kafka:9092\"\n",
    "kafka_topic = \"test\"\n",
    "\n",
    "# Define the schema for the JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"markers\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"lat\", FloatType(), nullable=False),\n",
    "            StructField(\"lng\", FloatType(), nullable=False)\n",
    "        ])\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Read data from Kafka as a streaming DataFrame\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic).load() \n",
    "    #.option(\"startingOffsets\", \"latest\") \\\n",
    "\n",
    "\n",
    "streaming_df = streaming_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Deserialize the JSON data using the specified schema\n",
    "streaming_df = streaming_df.select(from_json(\"value\", schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Extract individual fields from the array\n",
    "streaming_df = streaming_df.select(\n",
    "    col(\"markers\").getItem(0).getItem(\"lat\").alias(\"lat1\"),\n",
    "    col(\"markers\").getItem(0).getItem(\"lng\").alias(\"lng1\"),\n",
    "    col(\"markers\").getItem(1).getItem(\"lat\").alias(\"lat2\"),\n",
    "    col(\"markers\").getItem(1).getItem(\"lng\").alias(\"lng2\")\n",
    ")\n",
    "\n",
    "# Display the streaming DataFrame to the console\n",
    "query = streaming_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Wait for the streaming query to finish\n",
    "query.awaitTermination()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
